install.packages("cowsay")
setwd("D:/GitHub/BiodiversityDataGapTW/analyses/")
.packs <- c("httr", "jsonlite", "dplyr", "data.table", "stringr",
"ggplot2", "scales", "RColorBrewer", "tidyr", "tidyverse",
"sf", "parallel", "lwgeom")
sapply(.packs, require, character.only = T)
sessionInfo()
tbia.color_6 <- c("#3E5145", "#76A678", "#E5C851", "#E2A45F", "#F8E3C4", "#C75454")
# TBIA portal download ver20230612
## load in data
tbia <- fread("../../analyses/01.raw_data/ver4_ver20240410/tbia_6615ef5ccd1cf200219ad72d.csv",
sep = ",", colClasses = "character", encoding = "UTF-8")
# TBIA portal download ver20230612
## load in data
tbia <- fread("D:/GitHub/BiodiversityDataGapTW/analyses/01.raw_data/ver4_ver20240410/tbia_6615ef5ccd1cf200219ad72d.csv",
sep = ",", colClasses = "character", encoding = "UTF-8")
shiny::runApp('D:/GitHub/BiodiversityDataGapTW/shinyapp/eg/bioNPS/code')
install.packages("markdown")
library(markdown)
runApp('D:/GitHub/BiodiversityDataGapTW/shinyapp/eg/bioNPS/code')
dim(tbia) # 19,478,136 63
colnames(tbia)
format(object.size(tbia), units = "auto") # 16.7 Gb
## cut down tbia dt size for efficiency
tbia_cut <- tbia %>%
select("id", "datasetName", "occurrenceID", "rightsHolder")
## some data has no occurrenceID (assumed all distinct data)
tbia_blank_occID <- tbia_cut %>%
filter(.$occurrenceID == "") # 8,138 has no occurrenceID (all from iOcean)
tbia_blank_occID
table(tbia_blank_occID$rightsHolder)
table(tbia_blank_occID$datasetName)
table(tbia_blank_occID$rightsHolder)
table(tbia_blank_occID$datasetName)
tbia_omited_blank_occID <- tbia_cut %>%
filter(.$occurrenceID != "")
## counting & removing duplicate data
tbia_omited_blank_occID$rh_occID <- paste0(tbia_omited_blank_occID$rightsHolder, "_", tbia_omited_blank_occID$occurrenceID)
tbia_unique <- unique(tbia_omited_blank_occID, by = "rh_occID")
## Check -- just checking numbers & contents of duplicated records
n_occur <- data.table(table(tbia_omited_blank_occID$rh_occID)) # see counts of duplicated value
head(n_occ)
head(n_occur)
n_occur_morethan2 <- n_occur[n_occur$N > 1,] # 2320 rightsHolder_occurrenceID appeared twice or more
n_occur_morethan2
n_occur_morethan2_dN <- tbia_omited_blank_occID[tbia_omited_blank_occID$rh_occID %in% n_occur_morethan2$V1,] # 重複來自哪個資料集
table(n_occur_morethan2_dN$datasetName)
fwrite(n_occur_morethan2, "C:/Users/taibif/Desktop/n_occur_morethan2.csv",
row.names = F, quote = T)
tbia_dup <- tbia_omited_blank_occID[tbia_omited_blank_occID$rh_occID %in% n_occur_morethan2$V1,]
head(tbia_dup)
rm(n_occur,n_occur_morethan2,n_occur_morethan2_dN,tbia_blank_occID,tbia_cut,tbia_dup,tbia_omited_blank_occID,tbia_unique)
gv()
gc()
## Make recognizable ID to remove later
tbia$dtsN_rH <- paste0(tbia$datasetName, "_", tbia$rightsHolder)
## Final dataset number should be:
length(unique(tbia$datasetName)) # n = 1,968
## 8 duplicated datasets was found when grouping datasetName_rightsHolder
## Reason: Same datasets contributed by 2 different rightsHolder
tbia_all_dts <- tbia %>%
group_by(datasetName, rightsHolder) %>%
summarize(idp = first(dtsN_rH), numRecord = n()) %>%
ungroup() %>%
select(idp, datasetName, rightsHolder, numRecord) # n = 1,972
# Check -- check duplicated dataset
n_occur <- data.table(table(tbia_all_dts$datasetName))
sort(n_occur$N, decreasing = T)
dts_twice <- n_occur[n_occur$N == 2][[1]]
dts_twice
dup_dts <- tbia_all_dts[tbia_all_dts$datasetName %in% dts_twice,]
dup_dts
fwrite(dup_dts, "C:/Users/taibif/Desktop/tbia_clean2.0_check_datasetName_rightsHolder_duplicates.csv",
row.names = F, quote = T)
tbia_all_dts2 <- tbia_all_dts %>%
group_by(datasetName) %>%
filter(numRecord == max(numRecord)) %>% # choose row with higher record number
arrange(desc(numRecord), rightsHolder, .by_group = T) %>% # if identical, choose the first rightsHolder by alphabet
slice(1) %>%
ungroup() %>%
select(idp, datasetName, rightsHolder, numRecord)
21390501-647
# Keep one with the most data count, if identical: pick one
# total data count of the removed dataset = 647
# Final data count = 21390501-647 = 21,389,854
# the chosen dataset:
tbia_all_dts2 <- tbia_all_dts %>%
group_by(datasetName) %>%
filter(numRecord == max(numRecord)) %>% # choose row with higher record number
arrange(desc(numRecord), rightsHolder, .by_group = T) %>% # if identical, choose the first rightsHolder by alphabet
slice(1) %>%
ungroup() %>%
select(idp, datasetName, rightsHolder, numRecord)
tbia_all_dts2
# dataset to be removed, 4
dup_toRemove <- dup_dts %>%
group_by(datasetName) %>%
filter(numRecord == min(numRecord)) %>%
arrange(numRecord, rightsHolder, .by_group = T) %>%
slice(1) %>%
ungroup() %>%
select(idp, datasetName, rightsHolder, numRecord)
dup_toRemove
19+56+563+9
# remove from big table
tbia2 <- tbia[!tbia$dtsN_rH %in% dup_toRemove$idp, ]
tbia_clean <- tbia2[, -"dtsN_rH"]
dim(tbia_clean) # 21,389,854
### Final TBIA table
fwrite(tbia_clean, "D:/GitHub/BiodiversityDataGapTW/analyses/02.processed_data/tbia_v1_duplicates_removed.csv",
row.names = F, quote = T)
rm(list = ls(all.names = T))
gc()
tbia <- fread("D:/GitHub/BiodiversityDataGapTW/analyses/02.processed_data/tbia_v1_duplicates_removed.csv",
sep = ",", colClasses = "character", encoding = "UTF-8")
tbia$latitude <- fifelse(tbia$dataGeneralizations == "true", paste0(tbia$standardRawLatitude), paste0(tbia$standardLatitude))
tbia$longitude <- fifelse(tbia$dataGeneralizations == "true", paste0(tbia$standardRawLongitude), paste0(tbia$standardLongitude))
## Extract year from standardDate
tbia$year <- str_extract(tbia$standardDate, "\\d{4}")
tbia$month <- str_extract(tbia$standardDate, "(?<=-)[0-9]{2}(?=-)")
## Select columns needed for analysis only
keepColumn <- c("id", "rightsHolder", "datasetName", "basisOfRecord", "year", "month",
"latitude", "longitude", "coordinatePrecision", "coordinateUncertaintyInMeters", "dataGeneralizations",
"scientificNameID", "taxonID", "scientificName", "taxonRank", "common_name_c",
"kingdom", "kingdom_c", "phylum", "phylum_c", "class", "class_c",
"order", "order_c", "family", "family_c", "genus", "genus_c") # 27
tbia1 <- tbia %>%
select(all_of(keepColumn))
# Assign data quality
tbia1[tbia1 == ""] <- NA
tbia2 <- tbia1 %>%
mutate(dataQuality = case_when(
!is.na(scientificName) & !is.na(latitude) & !is.na(longitude) & !is.na(year) & !is.na(month) & !is.na(basisOfRecord) &
(!is.na(coordinatePrecision) | !is.na(coordinateUncertaintyInMeters)) ~ 'gold',
!is.na(scientificName) & !is.na(latitude) & !is.na(longitude) & !is.na(year) &
(!is.na(coordinatePrecision) | !is.na(coordinateUncertaintyInMeters)) ~ 'silver',
!is.na(scientificName) & !is.na(latitude) & !is.na(longitude) & !is.na(year) ~ 'bronze',
is.na(scientificName) | is.na(year) | is.na(longitude) | is.na(latitude) ~ 'low',
TRUE ~ NA_character_
))
table(tbia2$dataQuality)
fwrite(tbia2, "02.processed_data/tbia_v2_dataQuality.csv",
row.names = F, quote = T)
## check how many blank cells in each columns
blank_counts_test <- colSums(is.na(tbia2))
blank_counts_test
rm(list = ls(all.names = T))
gc()
tbia2 <- fread("../../analyses/02.processed_data/tbia_v2_dataQuality.csv",
sep = ",", colClasses = "character", encoding = "UTF-8")
tbia2 <- fread("D:/GitHub/BiodiversityDataGapTW/analyses/02.processed_data/tbia_v1_duplicates_removed.csv",
sep = ",", colClasses = "character", encoding = "UTF-8")
# Preparing coordinates
## remove data without coordinates
tbia2_withCoords <- tbia2 %>%
filter(longitude != "" & latitude != "") # 1,041,906 has no coordinates
names(tbia2)
# Preparing coordinates
## remove data without coordinates
tbia2_withCoords <- tbia2 %>%
filter(longitude != "" & latitude != "") # 1,041,906 has no coordinates
tbia <- fread("D:/GitHub/BiodiversityDataGapTW/analyses/02.processed_data/tbia_v1_duplicates_removed.csv",
sep = ",", colClasses = "character", encoding = "UTF-8")
gc()
tbia <- fread("D:/GitHub/BiodiversityDataGapTW/analyses/02.processed_data/tbia_v1_duplicates_removed.csv",
sep = ",", colClasses = "character", encoding = "UTF-8")
names(tbia)
tbia$latitude <- fifelse(tbia$dataGeneralizations == "true", paste0(tbia$standardRawLatitude), paste0(tbia$standardLatitude))
tbia$longitude <- fifelse(tbia$dataGeneralizations == "true", paste0(tbia$standardRawLongitude), paste0(tbia$standardLongitude))
## Extract year from standardDate
tbia$year <- str_extract(tbia$standardDate, "\\d{4}")
tbia$month <- str_extract(tbia$standardDate, "(?<=-)[0-9]{2}(?=-)")
## Select columns needed for analysis only
keepColumn <- c("id", "rightsHolder", "datasetName", "basisOfRecord", "year", "month",
"latitude", "longitude", "coordinatePrecision", "coordinateUncertaintyInMeters", "dataGeneralizations",
"scientificNameID", "taxonID", "scientificName", "taxonRank", "common_name_c",
"kingdom", "kingdom_c", "phylum", "phylum_c", "class", "class_c",
"order", "order_c", "family", "family_c", "genus", "genus_c") # 27
tbia1 <- tbia %>%
select(all_of(keepColumn))
# Assign data quality
tbia1[tbia1 == ""] <- NA
tbia2 <- tbia1 %>%
mutate(dataQuality = case_when(
!is.na(scientificName) & !is.na(latitude) & !is.na(longitude) & !is.na(year) & !is.na(month) & !is.na(basisOfRecord) &
(!is.na(coordinatePrecision) | !is.na(coordinateUncertaintyInMeters)) ~ 'gold',
!is.na(scientificName) & !is.na(latitude) & !is.na(longitude) & !is.na(year) &
(!is.na(coordinatePrecision) | !is.na(coordinateUncertaintyInMeters)) ~ 'silver',
!is.na(scientificName) & !is.na(latitude) & !is.na(longitude) & !is.na(year) ~ 'bronze',
is.na(scientificName) | is.na(year) | is.na(longitude) | is.na(latitude) ~ 'low',
TRUE ~ NA_character_
))
table(tbia2$dataQuality)
names(tbia2)
fwrite(tbia2, "D:/GitHub/BiodiversityDataGapTW/analyses/02.processed_data/tbia_v1_duplicates_removed.csv",
row.names = F, quote = T)
names(tbia2)
tbia <- fread("D:/GitHub/BiodiversityDataGapTW/analyses/02.processed_data/tbia_v1_duplicates_removed.csv",
sep = ",", colClasses = "character", encoding = "UTF-8")
names(tbia)
rm(tbia1, tbia2)
rm(keepColumn)
gc()
# Preparing coordinates
## remove data without coordinates
tbia2_withCoords <- tbia2 %>%
filter(longitude != "" & latitude != "") # 1,041,906 has no coordinates
# Preparing coordinates
## remove data without coordinates
tbia2_withCoords <- tbia %>%
filter(longitude != "" & latitude != "") # 1,041,906 has no coordinates
# Put points on Taiwan map
tbia2.1 <- tbia2_withCoords %>%
group_by(rightsHolder, latitude, longitude) %>%
count()
21389854-20595548
tbia2.2_groupList <- split(tbia2.1, tbia2.1$rightsHolder)
catchLocation <- function(x){
x %>%
st_as_sf(coords = c("longitude", "latitude"), remove = F) %>%
st_set_crs(4326) %>%
st_join(., shpFile, join = st_intersects, left = T, largest = T) %>%
st_drop_geometry(.)
}
